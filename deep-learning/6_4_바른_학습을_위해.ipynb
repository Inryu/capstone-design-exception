{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.4 바른 학습을 위해.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7DAUQpXvKCVNykA5p41eb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Inryu/capstone-design-exception/blob/main/deep-learning/6_4_%EB%B0%94%EB%A5%B8_%ED%95%99%EC%8A%B5%EC%9D%84_%EC%9C%84%ED%95%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-wwfyYxO0Ln"
      },
      "source": [
        "## __6.4 바른 학습을 위해__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVb5d2a6O4m9"
      },
      "source": [
        "### __6.4.1 오버피팅__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chCb2yw6O73y",
        "outputId": "e4854ccf-ae16-42f5-a51f-464efa936a3b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zvp30WHPQ0e",
        "outputId": "c1d12dde-c207-4ce7-aaf5-49a0456c0b44"
      },
      "source": [
        "%cd /content/drive/MyDrive/deep-learning-from-scratch-master/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/deep-learning-from-scratch-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-fs3k1oPaR3"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import SGD\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOL98kmWQR9b",
        "outputId": "a882ff7a-d336-48cd-ddaa-fb15b6e512f5"
      },
      "source": [
        "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10)\n",
        "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
        "\n",
        "max_epochs = 201\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(1000000000):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0, train acc:0.07333333333333333, test acc:0.1062\n",
            "epoch:1, train acc:0.09, test acc:0.1128\n",
            "epoch:2, train acc:0.1, test acc:0.1207\n",
            "epoch:3, train acc:0.12333333333333334, test acc:0.1297\n",
            "epoch:4, train acc:0.15666666666666668, test acc:0.1356\n",
            "epoch:5, train acc:0.18, test acc:0.1507\n",
            "epoch:6, train acc:0.21, test acc:0.1664\n",
            "epoch:7, train acc:0.24, test acc:0.1847\n",
            "epoch:8, train acc:0.26666666666666666, test acc:0.2016\n",
            "epoch:9, train acc:0.2966666666666667, test acc:0.2189\n",
            "epoch:10, train acc:0.32, test acc:0.2312\n",
            "epoch:11, train acc:0.3333333333333333, test acc:0.2455\n",
            "epoch:12, train acc:0.3466666666666667, test acc:0.2523\n",
            "epoch:13, train acc:0.34, test acc:0.2587\n",
            "epoch:14, train acc:0.36666666666666664, test acc:0.2681\n",
            "epoch:15, train acc:0.36666666666666664, test acc:0.2784\n",
            "epoch:16, train acc:0.39666666666666667, test acc:0.2881\n",
            "epoch:17, train acc:0.42333333333333334, test acc:0.2921\n",
            "epoch:18, train acc:0.43666666666666665, test acc:0.3074\n",
            "epoch:19, train acc:0.44, test acc:0.3148\n",
            "epoch:20, train acc:0.47, test acc:0.3394\n",
            "epoch:21, train acc:0.4866666666666667, test acc:0.3546\n",
            "epoch:22, train acc:0.5033333333333333, test acc:0.3704\n",
            "epoch:23, train acc:0.51, test acc:0.3907\n",
            "epoch:24, train acc:0.5166666666666667, test acc:0.4004\n",
            "epoch:25, train acc:0.52, test acc:0.407\n",
            "epoch:26, train acc:0.55, test acc:0.4262\n",
            "epoch:27, train acc:0.5766666666666667, test acc:0.4438\n",
            "epoch:28, train acc:0.5833333333333334, test acc:0.4539\n",
            "epoch:29, train acc:0.61, test acc:0.4697\n",
            "epoch:30, train acc:0.63, test acc:0.4694\n",
            "epoch:31, train acc:0.65, test acc:0.481\n",
            "epoch:32, train acc:0.66, test acc:0.4876\n",
            "epoch:33, train acc:0.6766666666666666, test acc:0.4981\n",
            "epoch:34, train acc:0.68, test acc:0.5122\n",
            "epoch:35, train acc:0.7, test acc:0.5169\n",
            "epoch:36, train acc:0.7166666666666667, test acc:0.5282\n",
            "epoch:37, train acc:0.7233333333333334, test acc:0.5334\n",
            "epoch:38, train acc:0.74, test acc:0.5449\n",
            "epoch:39, train acc:0.7533333333333333, test acc:0.5611\n",
            "epoch:40, train acc:0.7466666666666667, test acc:0.5644\n",
            "epoch:41, train acc:0.7533333333333333, test acc:0.5707\n",
            "epoch:42, train acc:0.7666666666666667, test acc:0.5803\n",
            "epoch:43, train acc:0.7833333333333333, test acc:0.5894\n",
            "epoch:44, train acc:0.8066666666666666, test acc:0.6007\n",
            "epoch:45, train acc:0.81, test acc:0.6106\n",
            "epoch:46, train acc:0.82, test acc:0.6208\n",
            "epoch:47, train acc:0.82, test acc:0.6228\n",
            "epoch:48, train acc:0.8233333333333334, test acc:0.6315\n",
            "epoch:49, train acc:0.8466666666666667, test acc:0.6232\n",
            "epoch:50, train acc:0.8433333333333334, test acc:0.6246\n",
            "epoch:51, train acc:0.84, test acc:0.6347\n",
            "epoch:52, train acc:0.84, test acc:0.6428\n",
            "epoch:53, train acc:0.8466666666666667, test acc:0.6422\n",
            "epoch:54, train acc:0.8466666666666667, test acc:0.6495\n",
            "epoch:55, train acc:0.8766666666666667, test acc:0.6523\n",
            "epoch:56, train acc:0.8666666666666667, test acc:0.6593\n",
            "epoch:57, train acc:0.8633333333333333, test acc:0.6574\n",
            "epoch:58, train acc:0.8733333333333333, test acc:0.6487\n",
            "epoch:59, train acc:0.87, test acc:0.6525\n",
            "epoch:60, train acc:0.89, test acc:0.6649\n",
            "epoch:61, train acc:0.8966666666666666, test acc:0.6722\n",
            "epoch:62, train acc:0.9, test acc:0.6761\n",
            "epoch:63, train acc:0.8966666666666666, test acc:0.6676\n",
            "epoch:64, train acc:0.8966666666666666, test acc:0.6682\n",
            "epoch:65, train acc:0.89, test acc:0.6859\n",
            "epoch:66, train acc:0.9133333333333333, test acc:0.6828\n",
            "epoch:67, train acc:0.9133333333333333, test acc:0.681\n",
            "epoch:68, train acc:0.8966666666666666, test acc:0.6768\n",
            "epoch:69, train acc:0.91, test acc:0.69\n",
            "epoch:70, train acc:0.93, test acc:0.6916\n",
            "epoch:71, train acc:0.9233333333333333, test acc:0.6882\n",
            "epoch:72, train acc:0.93, test acc:0.6963\n",
            "epoch:73, train acc:0.93, test acc:0.698\n",
            "epoch:74, train acc:0.93, test acc:0.6959\n",
            "epoch:75, train acc:0.93, test acc:0.6984\n",
            "epoch:76, train acc:0.9333333333333333, test acc:0.7001\n",
            "epoch:77, train acc:0.94, test acc:0.7048\n",
            "epoch:78, train acc:0.9366666666666666, test acc:0.6966\n",
            "epoch:79, train acc:0.9433333333333334, test acc:0.7032\n",
            "epoch:80, train acc:0.94, test acc:0.7095\n",
            "epoch:81, train acc:0.9433333333333334, test acc:0.708\n",
            "epoch:82, train acc:0.9566666666666667, test acc:0.7095\n",
            "epoch:83, train acc:0.9566666666666667, test acc:0.7106\n",
            "epoch:84, train acc:0.9566666666666667, test acc:0.7136\n",
            "epoch:85, train acc:0.9633333333333334, test acc:0.7153\n",
            "epoch:86, train acc:0.96, test acc:0.7169\n",
            "epoch:87, train acc:0.9566666666666667, test acc:0.7164\n",
            "epoch:88, train acc:0.96, test acc:0.7134\n",
            "epoch:89, train acc:0.96, test acc:0.7149\n",
            "epoch:90, train acc:0.96, test acc:0.722\n",
            "epoch:91, train acc:0.9633333333333334, test acc:0.7154\n",
            "epoch:92, train acc:0.9666666666666667, test acc:0.7225\n",
            "epoch:93, train acc:0.9733333333333334, test acc:0.7207\n",
            "epoch:94, train acc:0.97, test acc:0.7218\n",
            "epoch:95, train acc:0.9633333333333334, test acc:0.7218\n",
            "epoch:96, train acc:0.9733333333333334, test acc:0.7244\n",
            "epoch:97, train acc:0.9733333333333334, test acc:0.7295\n",
            "epoch:98, train acc:0.9766666666666667, test acc:0.7278\n",
            "epoch:99, train acc:0.9766666666666667, test acc:0.7317\n",
            "epoch:100, train acc:0.9766666666666667, test acc:0.7244\n",
            "epoch:101, train acc:0.9833333333333333, test acc:0.7258\n",
            "epoch:102, train acc:0.9833333333333333, test acc:0.7238\n",
            "epoch:103, train acc:0.99, test acc:0.7295\n",
            "epoch:104, train acc:0.99, test acc:0.7327\n",
            "epoch:105, train acc:0.9933333333333333, test acc:0.7313\n",
            "epoch:106, train acc:0.9933333333333333, test acc:0.7328\n",
            "epoch:107, train acc:0.9866666666666667, test acc:0.7329\n",
            "epoch:108, train acc:0.9833333333333333, test acc:0.7331\n",
            "epoch:109, train acc:0.9933333333333333, test acc:0.7338\n",
            "epoch:110, train acc:0.9933333333333333, test acc:0.7354\n",
            "epoch:111, train acc:0.9966666666666667, test acc:0.7364\n",
            "epoch:112, train acc:0.9933333333333333, test acc:0.7353\n",
            "epoch:113, train acc:0.99, test acc:0.737\n",
            "epoch:114, train acc:0.9933333333333333, test acc:0.7362\n",
            "epoch:115, train acc:0.9933333333333333, test acc:0.7359\n",
            "epoch:116, train acc:0.9933333333333333, test acc:0.7353\n",
            "epoch:117, train acc:0.99, test acc:0.7332\n",
            "epoch:118, train acc:0.9933333333333333, test acc:0.7366\n",
            "epoch:119, train acc:1.0, test acc:0.7337\n",
            "epoch:120, train acc:0.9966666666666667, test acc:0.7378\n",
            "epoch:121, train acc:0.9966666666666667, test acc:0.737\n",
            "epoch:122, train acc:1.0, test acc:0.737\n",
            "epoch:123, train acc:1.0, test acc:0.7392\n",
            "epoch:124, train acc:1.0, test acc:0.7389\n",
            "epoch:125, train acc:0.9966666666666667, test acc:0.7349\n",
            "epoch:126, train acc:0.9966666666666667, test acc:0.7385\n",
            "epoch:127, train acc:1.0, test acc:0.7366\n",
            "epoch:128, train acc:1.0, test acc:0.7395\n",
            "epoch:129, train acc:1.0, test acc:0.7412\n",
            "epoch:130, train acc:1.0, test acc:0.7376\n",
            "epoch:131, train acc:1.0, test acc:0.7403\n",
            "epoch:132, train acc:1.0, test acc:0.7412\n",
            "epoch:133, train acc:1.0, test acc:0.7409\n",
            "epoch:134, train acc:1.0, test acc:0.7414\n",
            "epoch:135, train acc:1.0, test acc:0.7425\n",
            "epoch:136, train acc:1.0, test acc:0.7411\n",
            "epoch:137, train acc:1.0, test acc:0.7419\n",
            "epoch:138, train acc:1.0, test acc:0.74\n",
            "epoch:139, train acc:1.0, test acc:0.7415\n",
            "epoch:140, train acc:1.0, test acc:0.7408\n",
            "epoch:141, train acc:1.0, test acc:0.7403\n",
            "epoch:142, train acc:1.0, test acc:0.7446\n",
            "epoch:143, train acc:1.0, test acc:0.7424\n",
            "epoch:144, train acc:1.0, test acc:0.7407\n",
            "epoch:145, train acc:1.0, test acc:0.7422\n",
            "epoch:146, train acc:1.0, test acc:0.7414\n",
            "epoch:147, train acc:1.0, test acc:0.7444\n",
            "epoch:148, train acc:1.0, test acc:0.7444\n",
            "epoch:149, train acc:1.0, test acc:0.743\n",
            "epoch:150, train acc:1.0, test acc:0.743\n",
            "epoch:151, train acc:1.0, test acc:0.7427\n",
            "epoch:152, train acc:1.0, test acc:0.7435\n",
            "epoch:153, train acc:1.0, test acc:0.7432\n",
            "epoch:154, train acc:1.0, test acc:0.741\n",
            "epoch:155, train acc:1.0, test acc:0.7426\n",
            "epoch:156, train acc:1.0, test acc:0.7418\n",
            "epoch:157, train acc:1.0, test acc:0.7443\n",
            "epoch:158, train acc:1.0, test acc:0.7451\n",
            "epoch:159, train acc:1.0, test acc:0.7444\n",
            "epoch:160, train acc:1.0, test acc:0.744\n",
            "epoch:161, train acc:1.0, test acc:0.743\n",
            "epoch:162, train acc:1.0, test acc:0.7441\n",
            "epoch:163, train acc:1.0, test acc:0.742\n",
            "epoch:164, train acc:1.0, test acc:0.7446\n",
            "epoch:165, train acc:1.0, test acc:0.7474\n",
            "epoch:166, train acc:1.0, test acc:0.7449\n",
            "epoch:167, train acc:1.0, test acc:0.7461\n",
            "epoch:168, train acc:1.0, test acc:0.7469\n",
            "epoch:169, train acc:1.0, test acc:0.7432\n",
            "epoch:170, train acc:1.0, test acc:0.7444\n",
            "epoch:171, train acc:1.0, test acc:0.7465\n",
            "epoch:172, train acc:1.0, test acc:0.7467\n",
            "epoch:173, train acc:1.0, test acc:0.7464\n",
            "epoch:174, train acc:1.0, test acc:0.7468\n",
            "epoch:175, train acc:1.0, test acc:0.7446\n",
            "epoch:176, train acc:1.0, test acc:0.7468\n",
            "epoch:177, train acc:1.0, test acc:0.7467\n",
            "epoch:178, train acc:1.0, test acc:0.7465\n",
            "epoch:179, train acc:1.0, test acc:0.7463\n",
            "epoch:180, train acc:1.0, test acc:0.7473\n",
            "epoch:181, train acc:1.0, test acc:0.7473\n",
            "epoch:182, train acc:1.0, test acc:0.746\n",
            "epoch:183, train acc:1.0, test acc:0.7454\n",
            "epoch:184, train acc:1.0, test acc:0.7473\n",
            "epoch:185, train acc:1.0, test acc:0.7467\n",
            "epoch:186, train acc:1.0, test acc:0.7485\n",
            "epoch:187, train acc:1.0, test acc:0.7471\n",
            "epoch:188, train acc:1.0, test acc:0.7469\n",
            "epoch:189, train acc:1.0, test acc:0.7491\n",
            "epoch:190, train acc:1.0, test acc:0.7479\n",
            "epoch:191, train acc:1.0, test acc:0.7454\n",
            "epoch:192, train acc:1.0, test acc:0.7456\n",
            "epoch:193, train acc:1.0, test acc:0.7488\n",
            "epoch:194, train acc:1.0, test acc:0.7473\n",
            "epoch:195, train acc:1.0, test acc:0.7484\n",
            "epoch:196, train acc:1.0, test acc:0.7478\n",
            "epoch:197, train acc:1.0, test acc:0.7464\n",
            "epoch:198, train acc:1.0, test acc:0.7482\n",
            "epoch:199, train acc:1.0, test acc:0.7499\n",
            "epoch:200, train acc:1.0, test acc:0.7502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "PdH_AuwXQ3dU",
        "outputId": "e64ca8d0-5ad2-43d0-82e0-55d5a6b917e6"
      },
      "source": [
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w89EvKclR1tU"
      },
      "source": [
        "### __6.4.3 드롭아웃__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U9DYZeBR4QC"
      },
      "source": [
        "class Dropout:\n",
        "  def __init__(self, dropout_ratio=0.5):\n",
        "    self.dropout_ratio = dropout_ratio\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x, train_flag=True):\n",
        "    if train_flg:\n",
        "      self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "      return x * self.mask\n",
        "    else:\n",
        "      return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "  def backward(self, dout):\n",
        "    return dout * self.mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5ih8vJ6VTXFy",
        "outputId": "8421f648-3fd1-4830-988b-be95076d4a8d"
      },
      "source": [
        "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
        "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
        "dropout_ratio = 0.2\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=301, mini_batch_size=100,\n",
        "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
        "trainer.train()\n",
        "\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss:2.3209164923315533\n",
            "=== epoch:1, train acc:0.12666666666666668, test acc:0.1023 ===\n",
            "train loss:2.30562212164764\n",
            "train loss:2.305538344439558\n",
            "train loss:2.3097776280305267\n",
            "=== epoch:2, train acc:0.12666666666666668, test acc:0.1025 ===\n",
            "train loss:2.2979162400331763\n",
            "train loss:2.2901416775917434\n",
            "train loss:2.295266293291834\n",
            "=== epoch:3, train acc:0.13, test acc:0.1038 ===\n",
            "train loss:2.288698837872696\n",
            "train loss:2.3012733853302914\n",
            "train loss:2.3031376388586287\n",
            "=== epoch:4, train acc:0.13, test acc:0.1046 ===\n",
            "train loss:2.2756798868176005\n",
            "train loss:2.289083674810895\n",
            "train loss:2.2827318970210255\n",
            "=== epoch:5, train acc:0.14, test acc:0.1072 ===\n",
            "train loss:2.281790326165276\n",
            "train loss:2.293309638311889\n",
            "train loss:2.305397813097324\n",
            "=== epoch:6, train acc:0.14666666666666667, test acc:0.1094 ===\n",
            "train loss:2.3082862243975044\n",
            "train loss:2.294036497702744\n",
            "train loss:2.289261660589129\n",
            "=== epoch:7, train acc:0.15333333333333332, test acc:0.1121 ===\n",
            "train loss:2.301333674042112\n",
            "train loss:2.2931329475492306\n",
            "train loss:2.294288320478933\n",
            "=== epoch:8, train acc:0.15333333333333332, test acc:0.1132 ===\n",
            "train loss:2.282361123267359\n",
            "train loss:2.278016303652487\n",
            "train loss:2.2903946505532486\n",
            "=== epoch:9, train acc:0.15333333333333332, test acc:0.1157 ===\n",
            "train loss:2.2756491691578797\n",
            "train loss:2.296713366246173\n",
            "train loss:2.285797008992292\n",
            "=== epoch:10, train acc:0.15666666666666668, test acc:0.1167 ===\n",
            "train loss:2.2848552531055626\n",
            "train loss:2.282374031691224\n",
            "train loss:2.2709790748606737\n",
            "=== epoch:11, train acc:0.16, test acc:0.1174 ===\n",
            "train loss:2.2829760459250106\n",
            "train loss:2.275232126685335\n",
            "train loss:2.2794123873282683\n",
            "=== epoch:12, train acc:0.16666666666666666, test acc:0.1205 ===\n",
            "train loss:2.272045195823281\n",
            "train loss:2.2811365434867\n",
            "train loss:2.2841161860402597\n",
            "=== epoch:13, train acc:0.17333333333333334, test acc:0.1221 ===\n",
            "train loss:2.2761635119741994\n",
            "train loss:2.285992835741712\n",
            "train loss:2.291351409372862\n",
            "=== epoch:14, train acc:0.17333333333333334, test acc:0.1238 ===\n",
            "train loss:2.276683623398648\n",
            "train loss:2.2856830962036843\n",
            "train loss:2.2730541758793947\n",
            "=== epoch:15, train acc:0.2, test acc:0.1258 ===\n",
            "train loss:2.2854166529015383\n",
            "train loss:2.289307636377436\n",
            "train loss:2.2756759163968394\n",
            "=== epoch:16, train acc:0.20333333333333334, test acc:0.1284 ===\n",
            "train loss:2.2722565213570443\n",
            "train loss:2.277947410950367\n",
            "train loss:2.2752186476586953\n",
            "=== epoch:17, train acc:0.21333333333333335, test acc:0.1312 ===\n",
            "train loss:2.2690829902690934\n",
            "train loss:2.2761476091982225\n",
            "train loss:2.2785829716757724\n",
            "=== epoch:18, train acc:0.21333333333333335, test acc:0.1338 ===\n",
            "train loss:2.280065040065307\n",
            "train loss:2.286466432406446\n",
            "train loss:2.284285008185087\n",
            "=== epoch:19, train acc:0.21666666666666667, test acc:0.1398 ===\n",
            "train loss:2.2807752799811793\n",
            "train loss:2.2700800236971035\n",
            "train loss:2.2706100384087824\n",
            "=== epoch:20, train acc:0.22, test acc:0.1416 ===\n",
            "train loss:2.266450944481976\n",
            "train loss:2.2847012972940606\n",
            "train loss:2.267231145276237\n",
            "=== epoch:21, train acc:0.23333333333333334, test acc:0.1444 ===\n",
            "train loss:2.270270988764294\n",
            "train loss:2.2721200139648343\n",
            "train loss:2.274332465480606\n",
            "=== epoch:22, train acc:0.24333333333333335, test acc:0.1502 ===\n",
            "train loss:2.2742032307003583\n",
            "train loss:2.2547123302670253\n",
            "train loss:2.2752603201279484\n",
            "=== epoch:23, train acc:0.24666666666666667, test acc:0.1586 ===\n",
            "train loss:2.2748741741666563\n",
            "train loss:2.2650314701632484\n",
            "train loss:2.2818938751357063\n",
            "=== epoch:24, train acc:0.25333333333333335, test acc:0.162 ===\n",
            "train loss:2.268372793774189\n",
            "train loss:2.274875354389354\n",
            "train loss:2.26167748487134\n",
            "=== epoch:25, train acc:0.27, test acc:0.1637 ===\n",
            "train loss:2.256277541175521\n",
            "train loss:2.271918354815685\n",
            "train loss:2.261742702706866\n",
            "=== epoch:26, train acc:0.2733333333333333, test acc:0.1671 ===\n",
            "train loss:2.2674643636445313\n",
            "train loss:2.27151194224153\n",
            "train loss:2.254814055863934\n",
            "=== epoch:27, train acc:0.27, test acc:0.1714 ===\n",
            "train loss:2.264262445572768\n",
            "train loss:2.247507751461397\n",
            "train loss:2.2575282601559796\n",
            "=== epoch:28, train acc:0.27, test acc:0.1759 ===\n",
            "train loss:2.2583777164310472\n",
            "train loss:2.2760779921668854\n",
            "train loss:2.268814366703584\n",
            "=== epoch:29, train acc:0.27666666666666667, test acc:0.1856 ===\n",
            "train loss:2.2543395168539444\n",
            "train loss:2.2604054655291494\n",
            "train loss:2.2644961080032506\n",
            "=== epoch:30, train acc:0.2733333333333333, test acc:0.1933 ===\n",
            "train loss:2.2558192278122795\n",
            "train loss:2.2542271095788733\n",
            "train loss:2.2522933189661725\n",
            "=== epoch:31, train acc:0.28, test acc:0.1923 ===\n",
            "train loss:2.261342474297675\n",
            "train loss:2.2710361733769187\n",
            "train loss:2.2524692506445807\n",
            "=== epoch:32, train acc:0.2833333333333333, test acc:0.2011 ===\n",
            "train loss:2.2469952539387346\n",
            "train loss:2.269456585228595\n",
            "train loss:2.2528065281519707\n",
            "=== epoch:33, train acc:0.3, test acc:0.2071 ===\n",
            "train loss:2.2586486412877242\n",
            "train loss:2.2559080021698237\n",
            "train loss:2.240925831052679\n",
            "=== epoch:34, train acc:0.31666666666666665, test acc:0.2125 ===\n",
            "train loss:2.2647785329814\n",
            "train loss:2.2583626229181473\n",
            "train loss:2.252084878217385\n",
            "=== epoch:35, train acc:0.30333333333333334, test acc:0.2184 ===\n",
            "train loss:2.270966804439064\n",
            "train loss:2.259615202488025\n",
            "train loss:2.2578027877638416\n",
            "=== epoch:36, train acc:0.31666666666666665, test acc:0.224 ===\n",
            "train loss:2.2405636669368776\n",
            "train loss:2.262349099098962\n",
            "train loss:2.245548051262025\n",
            "=== epoch:37, train acc:0.31333333333333335, test acc:0.228 ===\n",
            "train loss:2.2588460021687844\n",
            "train loss:2.2555110360288917\n",
            "train loss:2.2497127965641464\n",
            "=== epoch:38, train acc:0.33, test acc:0.2342 ===\n",
            "train loss:2.2399332435588963\n",
            "train loss:2.2429339777493023\n",
            "train loss:2.233067622567742\n",
            "=== epoch:39, train acc:0.34, test acc:0.2407 ===\n",
            "train loss:2.2431948548114446\n",
            "train loss:2.260452342491303\n",
            "train loss:2.236919942615705\n",
            "=== epoch:40, train acc:0.34, test acc:0.2478 ===\n",
            "train loss:2.248418462048615\n",
            "train loss:2.2459890527541297\n",
            "train loss:2.2566352283441065\n",
            "=== epoch:41, train acc:0.3466666666666667, test acc:0.2492 ===\n",
            "train loss:2.2439834122731868\n",
            "train loss:2.228747474657642\n",
            "train loss:2.2501705098991382\n",
            "=== epoch:42, train acc:0.3566666666666667, test acc:0.2577 ===\n",
            "train loss:2.240920601341389\n",
            "train loss:2.2499911422096095\n",
            "train loss:2.2383996412786695\n",
            "=== epoch:43, train acc:0.35333333333333333, test acc:0.2628 ===\n",
            "train loss:2.2489972340605178\n",
            "train loss:2.239920081362213\n",
            "train loss:2.2561822821381763\n",
            "=== epoch:44, train acc:0.3566666666666667, test acc:0.2609 ===\n",
            "train loss:2.2248695558933425\n",
            "train loss:2.2606158316657887\n",
            "train loss:2.2530400046768015\n",
            "=== epoch:45, train acc:0.3566666666666667, test acc:0.2655 ===\n",
            "train loss:2.2383505266808097\n",
            "train loss:2.253156957554809\n",
            "train loss:2.233175229576808\n",
            "=== epoch:46, train acc:0.3566666666666667, test acc:0.2641 ===\n",
            "train loss:2.220936385100594\n",
            "train loss:2.240914640755088\n",
            "train loss:2.2405165978675536\n",
            "=== epoch:47, train acc:0.35333333333333333, test acc:0.2679 ===\n",
            "train loss:2.234402153093626\n",
            "train loss:2.2411846337927877\n",
            "train loss:2.228405333227313\n",
            "=== epoch:48, train acc:0.36333333333333334, test acc:0.2733 ===\n",
            "train loss:2.244176860393481\n",
            "train loss:2.222332246853608\n",
            "train loss:2.223663402417439\n",
            "=== epoch:49, train acc:0.3566666666666667, test acc:0.2749 ===\n",
            "train loss:2.228779830375426\n",
            "train loss:2.2187984919384682\n",
            "train loss:2.2683796841929733\n",
            "=== epoch:50, train acc:0.36, test acc:0.2769 ===\n",
            "train loss:2.2331068146423663\n",
            "train loss:2.2281456004184887\n",
            "train loss:2.245299044056564\n",
            "=== epoch:51, train acc:0.37, test acc:0.2836 ===\n",
            "train loss:2.2256078982733407\n",
            "train loss:2.2319169279357927\n",
            "train loss:2.211274821020882\n",
            "=== epoch:52, train acc:0.37666666666666665, test acc:0.287 ===\n",
            "train loss:2.2196974198887887\n",
            "train loss:2.223515777357959\n",
            "train loss:2.2311078576455423\n",
            "=== epoch:53, train acc:0.37666666666666665, test acc:0.2897 ===\n",
            "train loss:2.243722358883437\n",
            "train loss:2.2315422136389063\n",
            "train loss:2.220186490337746\n",
            "=== epoch:54, train acc:0.3933333333333333, test acc:0.292 ===\n",
            "train loss:2.2222378557660596\n",
            "train loss:2.240219141459679\n",
            "train loss:2.2104167704114217\n",
            "=== epoch:55, train acc:0.4033333333333333, test acc:0.2946 ===\n",
            "train loss:2.222996180279515\n",
            "train loss:2.228171852916538\n",
            "train loss:2.239332548322931\n",
            "=== epoch:56, train acc:0.41, test acc:0.2981 ===\n",
            "train loss:2.2354379206721102\n",
            "train loss:2.2214487230601354\n",
            "train loss:2.2074550027561175\n",
            "=== epoch:57, train acc:0.42, test acc:0.3029 ===\n",
            "train loss:2.2171039598609874\n",
            "train loss:2.21863093164765\n",
            "train loss:2.2133974519788056\n",
            "=== epoch:58, train acc:0.4, test acc:0.304 ===\n",
            "train loss:2.224050885780092\n",
            "train loss:2.2194551400300027\n",
            "train loss:2.213910948021778\n",
            "=== epoch:59, train acc:0.4, test acc:0.3047 ===\n",
            "train loss:2.230260525884519\n",
            "train loss:2.221959959820201\n",
            "train loss:2.2185605950466734\n",
            "=== epoch:60, train acc:0.41333333333333333, test acc:0.3104 ===\n",
            "train loss:2.2291461314860537\n",
            "train loss:2.2115281017302046\n",
            "train loss:2.2157925389134734\n",
            "=== epoch:61, train acc:0.4166666666666667, test acc:0.3125 ===\n",
            "train loss:2.2269774908036992\n",
            "train loss:2.1986017772290865\n",
            "train loss:2.194970340349951\n",
            "=== epoch:62, train acc:0.4166666666666667, test acc:0.3115 ===\n",
            "train loss:2.226013588148643\n",
            "train loss:2.219649744295149\n",
            "train loss:2.230978158239902\n",
            "=== epoch:63, train acc:0.4266666666666667, test acc:0.3159 ===\n",
            "train loss:2.2289052387048978\n",
            "train loss:2.2105851616124865\n",
            "train loss:2.2087458376668296\n",
            "=== epoch:64, train acc:0.43333333333333335, test acc:0.3215 ===\n",
            "train loss:2.187692650379788\n",
            "train loss:2.2120593677687466\n",
            "train loss:2.20557866276989\n",
            "=== epoch:65, train acc:0.43666666666666665, test acc:0.3172 ===\n",
            "train loss:2.2133437625434986\n",
            "train loss:2.214016359481313\n",
            "train loss:2.2159473379146517\n",
            "=== epoch:66, train acc:0.43333333333333335, test acc:0.3219 ===\n",
            "train loss:2.1967392293960715\n",
            "train loss:2.2201232294955755\n",
            "train loss:2.179226155385486\n",
            "=== epoch:67, train acc:0.43666666666666665, test acc:0.3236 ===\n",
            "train loss:2.211004805734572\n",
            "train loss:2.211494812685622\n",
            "train loss:2.208767717693619\n",
            "=== epoch:68, train acc:0.44, test acc:0.324 ===\n",
            "train loss:2.212401214698731\n",
            "train loss:2.1667156642626146\n",
            "train loss:2.1879927742727543\n",
            "=== epoch:69, train acc:0.43333333333333335, test acc:0.3215 ===\n",
            "train loss:2.1987506015552216\n",
            "train loss:2.2091750241113886\n",
            "train loss:2.212123705039477\n",
            "=== epoch:70, train acc:0.43, test acc:0.3217 ===\n",
            "train loss:2.214702344334122\n",
            "train loss:2.2033738315192495\n",
            "train loss:2.187231700568567\n",
            "=== epoch:71, train acc:0.43666666666666665, test acc:0.3225 ===\n",
            "train loss:2.1816623288866763\n",
            "train loss:2.201003259776939\n",
            "train loss:2.1683619358140733\n",
            "=== epoch:72, train acc:0.4266666666666667, test acc:0.3203 ===\n",
            "train loss:2.2066755168657624\n",
            "train loss:2.190387395837719\n",
            "train loss:2.189021505874086\n",
            "=== epoch:73, train acc:0.4266666666666667, test acc:0.3172 ===\n",
            "train loss:2.1945936014109755\n",
            "train loss:2.215457290446299\n",
            "train loss:2.1887169999061182\n",
            "=== epoch:74, train acc:0.43, test acc:0.3212 ===\n",
            "train loss:2.1944912619148287\n",
            "train loss:2.1882259377530264\n",
            "train loss:2.223693143286759\n",
            "=== epoch:75, train acc:0.44, test acc:0.3252 ===\n",
            "train loss:2.1996685807131713\n",
            "train loss:2.1930360375468405\n",
            "train loss:2.193480834633808\n",
            "=== epoch:76, train acc:0.45, test acc:0.336 ===\n",
            "train loss:2.1836210274661587\n",
            "train loss:2.1855734552037225\n",
            "train loss:2.1789003905223323\n",
            "=== epoch:77, train acc:0.4533333333333333, test acc:0.3329 ===\n",
            "train loss:2.192078663775396\n",
            "train loss:2.145622543192034\n",
            "train loss:2.1927674888350936\n",
            "=== epoch:78, train acc:0.45, test acc:0.3374 ===\n",
            "train loss:2.1868504178221535\n",
            "train loss:2.2009784973310595\n",
            "train loss:2.214458484905103\n",
            "=== epoch:79, train acc:0.45, test acc:0.3415 ===\n",
            "train loss:2.1631080781769088\n",
            "train loss:2.183678453562301\n",
            "train loss:2.1810695919516165\n",
            "=== epoch:80, train acc:0.45, test acc:0.3415 ===\n",
            "train loss:2.1849135380087295\n",
            "train loss:2.164678024161663\n",
            "train loss:2.1501648085569274\n",
            "=== epoch:81, train acc:0.45, test acc:0.3381 ===\n",
            "train loss:2.1712170199493266\n",
            "train loss:2.15896255956782\n",
            "train loss:2.190645730907144\n",
            "=== epoch:82, train acc:0.4533333333333333, test acc:0.335 ===\n",
            "train loss:2.1858362862744225\n",
            "train loss:2.153909312359568\n",
            "train loss:2.180106273282021\n",
            "=== epoch:83, train acc:0.4533333333333333, test acc:0.3343 ===\n",
            "train loss:2.1727025570871397\n",
            "train loss:2.1961620943116897\n",
            "train loss:2.148328383204302\n",
            "=== epoch:84, train acc:0.4533333333333333, test acc:0.3332 ===\n",
            "train loss:2.174109172261104\n",
            "train loss:2.154948361101235\n",
            "train loss:2.1568561413494307\n",
            "=== epoch:85, train acc:0.44666666666666666, test acc:0.3251 ===\n",
            "train loss:2.165760079491807\n",
            "train loss:2.1488591994636033\n",
            "train loss:2.1588954917037144\n",
            "=== epoch:86, train acc:0.44333333333333336, test acc:0.3242 ===\n",
            "train loss:2.173339216921084\n",
            "train loss:2.188977872979536\n",
            "train loss:2.150507296813012\n",
            "=== epoch:87, train acc:0.44666666666666666, test acc:0.3284 ===\n",
            "train loss:2.1042053210757543\n",
            "train loss:2.1286196018026353\n",
            "train loss:2.171965327000483\n",
            "=== epoch:88, train acc:0.44333333333333336, test acc:0.3271 ===\n",
            "train loss:2.1667061242005086\n",
            "train loss:2.1510250175456718\n",
            "train loss:2.1607688262043308\n",
            "=== epoch:89, train acc:0.44666666666666666, test acc:0.3305 ===\n",
            "train loss:2.140856209923558\n",
            "train loss:2.1519821746484995\n",
            "train loss:2.1421690160208335\n",
            "=== epoch:90, train acc:0.43, test acc:0.3267 ===\n",
            "train loss:2.1649476647080768\n",
            "train loss:2.128435894067495\n",
            "train loss:2.1741725109734626\n",
            "=== epoch:91, train acc:0.43333333333333335, test acc:0.3266 ===\n",
            "train loss:2.1472957505901373\n",
            "train loss:2.168880694337651\n",
            "train loss:2.157307939318788\n",
            "=== epoch:92, train acc:0.44, test acc:0.3255 ===\n",
            "train loss:2.1419603565354053\n",
            "train loss:2.1544805165706715\n",
            "train loss:2.150366789869308\n",
            "=== epoch:93, train acc:0.44333333333333336, test acc:0.329 ===\n",
            "train loss:2.1310525922466876\n",
            "train loss:2.1374388191114453\n",
            "train loss:2.1793601074660534\n",
            "=== epoch:94, train acc:0.44333333333333336, test acc:0.3307 ===\n",
            "train loss:2.0979157717607047\n",
            "train loss:2.144038085733961\n",
            "train loss:2.1351275403402097\n",
            "=== epoch:95, train acc:0.43666666666666665, test acc:0.3229 ===\n",
            "train loss:2.115971558238875\n",
            "train loss:2.1235789603883943\n",
            "train loss:2.161936060276318\n",
            "=== epoch:96, train acc:0.4266666666666667, test acc:0.3211 ===\n",
            "train loss:2.156111041102071\n",
            "train loss:2.154662230334444\n",
            "train loss:2.138329177095813\n",
            "=== epoch:97, train acc:0.43, test acc:0.3245 ===\n",
            "train loss:2.158262819317404\n",
            "train loss:2.0993723091469\n",
            "train loss:2.121747939378717\n",
            "=== epoch:98, train acc:0.43, test acc:0.3215 ===\n",
            "train loss:2.1465222571153224\n",
            "train loss:2.1263662374679564\n",
            "train loss:2.1353353601009335\n",
            "=== epoch:99, train acc:0.43666666666666665, test acc:0.327 ===\n",
            "train loss:2.1309559183374343\n",
            "train loss:2.123365515404241\n",
            "train loss:2.1434401115104746\n",
            "=== epoch:100, train acc:0.44, test acc:0.3299 ===\n",
            "train loss:2.1440994047880304\n",
            "train loss:2.1084273734403296\n",
            "train loss:2.140522595868431\n",
            "=== epoch:101, train acc:0.43333333333333335, test acc:0.3274 ===\n",
            "train loss:2.141668767768638\n",
            "train loss:2.102691792881767\n",
            "train loss:2.114307479073869\n",
            "=== epoch:102, train acc:0.44333333333333336, test acc:0.33 ===\n",
            "train loss:2.157036121592967\n",
            "train loss:2.132059924011653\n",
            "train loss:2.1147598401829133\n",
            "=== epoch:103, train acc:0.43, test acc:0.3281 ===\n",
            "train loss:2.1000527958256283\n",
            "train loss:2.0664240226673125\n",
            "train loss:2.116474838635799\n",
            "=== epoch:104, train acc:0.42333333333333334, test acc:0.3246 ===\n",
            "train loss:2.117061517783186\n",
            "train loss:2.131790747069323\n",
            "train loss:2.1203140144889336\n",
            "=== epoch:105, train acc:0.43, test acc:0.328 ===\n",
            "train loss:2.092482664140822\n",
            "train loss:2.128086900207101\n",
            "train loss:2.144635413983892\n",
            "=== epoch:106, train acc:0.43333333333333335, test acc:0.3309 ===\n",
            "train loss:2.144125763527012\n",
            "train loss:2.117297874003994\n",
            "train loss:2.0667393090903095\n",
            "=== epoch:107, train acc:0.43333333333333335, test acc:0.3311 ===\n",
            "train loss:2.1224584931099617\n",
            "train loss:2.1197822916719447\n",
            "train loss:2.120707537533589\n",
            "=== epoch:108, train acc:0.44666666666666666, test acc:0.34 ===\n",
            "train loss:2.142929362612073\n",
            "train loss:2.1168705295958823\n",
            "train loss:2.0991635775753306\n",
            "=== epoch:109, train acc:0.4633333333333333, test acc:0.3491 ===\n",
            "train loss:2.064954041491503\n",
            "train loss:2.107497847173984\n",
            "train loss:2.1318012449042936\n",
            "=== epoch:110, train acc:0.4533333333333333, test acc:0.3482 ===\n",
            "train loss:2.05148434531215\n",
            "train loss:2.0945206915211587\n",
            "train loss:2.1304596757223475\n",
            "=== epoch:111, train acc:0.46, test acc:0.3482 ===\n",
            "train loss:2.039645525226165\n",
            "train loss:2.101774639450467\n",
            "train loss:2.093746839321448\n",
            "=== epoch:112, train acc:0.4666666666666667, test acc:0.3498 ===\n",
            "train loss:2.053037579455428\n",
            "train loss:2.124430968606383\n",
            "train loss:2.0830455218347743\n",
            "=== epoch:113, train acc:0.4633333333333333, test acc:0.3567 ===\n",
            "train loss:2.1201088361405036\n",
            "train loss:2.1408164082181975\n",
            "train loss:2.07657749930463\n",
            "=== epoch:114, train acc:0.49, test acc:0.3689 ===\n",
            "train loss:2.1052871036089913\n",
            "train loss:2.112857193033516\n",
            "train loss:2.1147294049728997\n",
            "=== epoch:115, train acc:0.49333333333333335, test acc:0.3781 ===\n",
            "train loss:2.072074910194503\n",
            "train loss:2.078290009670394\n",
            "train loss:2.0461475369770565\n",
            "=== epoch:116, train acc:0.49333333333333335, test acc:0.3713 ===\n",
            "train loss:2.098960579393023\n",
            "train loss:2.0894048715352675\n",
            "train loss:2.065418860770686\n",
            "=== epoch:117, train acc:0.49333333333333335, test acc:0.3666 ===\n",
            "train loss:2.1127507663557936\n",
            "train loss:2.0326114870601986\n",
            "train loss:2.074879846673593\n",
            "=== epoch:118, train acc:0.49333333333333335, test acc:0.3711 ===\n",
            "train loss:2.0670537479246134\n",
            "train loss:2.070636943319401\n",
            "train loss:2.043150077238544\n",
            "=== epoch:119, train acc:0.49666666666666665, test acc:0.3754 ===\n",
            "train loss:2.072941407163417\n",
            "train loss:2.076524544829131\n",
            "train loss:2.079709100545424\n",
            "=== epoch:120, train acc:0.5033333333333333, test acc:0.3821 ===\n",
            "train loss:2.061423636803614\n",
            "train loss:2.0715108011024705\n",
            "train loss:2.082163281551593\n",
            "=== epoch:121, train acc:0.5066666666666667, test acc:0.3896 ===\n",
            "train loss:2.038115377043737\n",
            "train loss:2.0565299121435294\n",
            "train loss:2.0664512621288815\n",
            "=== epoch:122, train acc:0.49333333333333335, test acc:0.3883 ===\n",
            "train loss:2.0505170264051524\n",
            "train loss:2.0941271528188996\n",
            "train loss:2.042003504038284\n",
            "=== epoch:123, train acc:0.5033333333333333, test acc:0.3905 ===\n",
            "train loss:2.033667180298286\n",
            "train loss:2.0376430303239745\n",
            "train loss:2.0718927054470977\n",
            "=== epoch:124, train acc:0.49666666666666665, test acc:0.3859 ===\n",
            "train loss:2.0560018993825984\n",
            "train loss:2.046599859427398\n",
            "train loss:2.020550213555243\n",
            "=== epoch:125, train acc:0.49666666666666665, test acc:0.3852 ===\n",
            "train loss:2.0622736678215396\n",
            "train loss:2.006830674362278\n",
            "train loss:2.0246180790088997\n",
            "=== epoch:126, train acc:0.49, test acc:0.381 ===\n",
            "train loss:2.012674936672165\n",
            "train loss:2.011945233256447\n",
            "train loss:2.04402891864549\n",
            "=== epoch:127, train acc:0.49333333333333335, test acc:0.3828 ===\n",
            "train loss:2.0981925711904252\n",
            "train loss:2.0585929271022905\n",
            "train loss:2.098243013071874\n",
            "=== epoch:128, train acc:0.5033333333333333, test acc:0.3946 ===\n",
            "train loss:2.0150018078830287\n",
            "train loss:2.095313228944519\n",
            "train loss:2.0629081227403367\n",
            "=== epoch:129, train acc:0.5, test acc:0.3956 ===\n",
            "train loss:2.049922721937099\n",
            "train loss:2.018643423514962\n",
            "train loss:2.0043751324487933\n",
            "=== epoch:130, train acc:0.5033333333333333, test acc:0.3941 ===\n",
            "train loss:2.015182195086761\n",
            "train loss:2.0173522637289336\n",
            "train loss:2.0352491316550108\n",
            "=== epoch:131, train acc:0.49666666666666665, test acc:0.3917 ===\n",
            "train loss:2.0171098016962623\n",
            "train loss:2.0543632625357398\n",
            "train loss:2.0154260009841227\n",
            "=== epoch:132, train acc:0.5033333333333333, test acc:0.396 ===\n",
            "train loss:1.9833538594745574\n",
            "train loss:2.001836442888499\n",
            "train loss:2.0029164000905193\n",
            "=== epoch:133, train acc:0.49666666666666665, test acc:0.3922 ===\n",
            "train loss:2.0384667442501665\n",
            "train loss:2.0375667081769326\n",
            "train loss:1.9966846062863228\n",
            "=== epoch:134, train acc:0.5, test acc:0.4007 ===\n",
            "train loss:2.028977966342754\n",
            "train loss:1.971084171333534\n",
            "train loss:1.9709695567812946\n",
            "=== epoch:135, train acc:0.5066666666666667, test acc:0.3997 ===\n",
            "train loss:2.0195017317397914\n",
            "train loss:1.9836127267646912\n",
            "train loss:2.0249194827398305\n",
            "=== epoch:136, train acc:0.5066666666666667, test acc:0.3995 ===\n",
            "train loss:1.9962328709321282\n",
            "train loss:1.956688385149322\n",
            "train loss:2.0159478879873145\n",
            "=== epoch:137, train acc:0.49666666666666665, test acc:0.3945 ===\n",
            "train loss:2.004956266832395\n",
            "train loss:1.9697454488139916\n",
            "train loss:1.981859929459913\n",
            "=== epoch:138, train acc:0.49666666666666665, test acc:0.3962 ===\n",
            "train loss:1.9624789654112176\n",
            "train loss:2.0201728295595998\n",
            "train loss:1.960531737800193\n",
            "=== epoch:139, train acc:0.49333333333333335, test acc:0.393 ===\n",
            "train loss:1.960385983885771\n",
            "train loss:1.9437190530826103\n",
            "train loss:2.0399855957859954\n",
            "=== epoch:140, train acc:0.5033333333333333, test acc:0.4006 ===\n",
            "train loss:1.9586272200214916\n",
            "train loss:1.9179922183736926\n",
            "train loss:1.9290160634998232\n",
            "=== epoch:141, train acc:0.5, test acc:0.3982 ===\n",
            "train loss:1.9687207124770147\n",
            "train loss:1.9716414774706041\n",
            "train loss:1.9321193936615857\n",
            "=== epoch:142, train acc:0.5, test acc:0.4002 ===\n",
            "train loss:1.9862861260863527\n",
            "train loss:1.9913304860307235\n",
            "train loss:1.9401919783234483\n",
            "=== epoch:143, train acc:0.5066666666666667, test acc:0.4104 ===\n",
            "train loss:1.9437420513385089\n",
            "train loss:1.935554938879313\n",
            "train loss:2.0175955443730498\n",
            "=== epoch:144, train acc:0.5133333333333333, test acc:0.411 ===\n",
            "train loss:1.9843707704132603\n",
            "train loss:1.9518522098096696\n",
            "train loss:1.9489047515851725\n",
            "=== epoch:145, train acc:0.5166666666666667, test acc:0.4132 ===\n",
            "train loss:1.9857072448700561\n",
            "train loss:1.912835796134401\n",
            "train loss:1.9665986923530037\n",
            "=== epoch:146, train acc:0.5033333333333333, test acc:0.4072 ===\n",
            "train loss:1.8753644330508066\n",
            "train loss:1.8979117996308466\n",
            "train loss:1.9433959191664931\n",
            "=== epoch:147, train acc:0.5033333333333333, test acc:0.4058 ===\n",
            "train loss:1.9788429029215595\n",
            "train loss:1.927586463471877\n",
            "train loss:2.0028619647274506\n",
            "=== epoch:148, train acc:0.5, test acc:0.4122 ===\n",
            "train loss:1.983656566682037\n",
            "train loss:1.948426642023099\n",
            "train loss:1.9998707617331406\n",
            "=== epoch:149, train acc:0.5166666666666667, test acc:0.42 ===\n",
            "train loss:1.9344726737724942\n",
            "train loss:1.9139908447706224\n",
            "train loss:2.0048710767424067\n",
            "=== epoch:150, train acc:0.5133333333333333, test acc:0.423 ===\n",
            "train loss:1.9190775884712354\n",
            "train loss:1.9779559808824287\n",
            "train loss:1.9401180890555705\n",
            "=== epoch:151, train acc:0.52, test acc:0.4227 ===\n",
            "train loss:1.962281010761252\n",
            "train loss:1.9417190073414654\n",
            "train loss:1.9440727297498748\n",
            "=== epoch:152, train acc:0.52, test acc:0.4262 ===\n",
            "train loss:1.9267839939764329\n",
            "train loss:1.9852425574601336\n",
            "train loss:1.944808666202527\n",
            "=== epoch:153, train acc:0.5233333333333333, test acc:0.4278 ===\n",
            "train loss:1.8395040585776194\n",
            "train loss:1.906307576162875\n",
            "train loss:1.9519434512705416\n",
            "=== epoch:154, train acc:0.52, test acc:0.4315 ===\n",
            "train loss:2.010654460615828\n",
            "train loss:1.9801721408732456\n",
            "train loss:1.9578217319973772\n",
            "=== epoch:155, train acc:0.5266666666666666, test acc:0.4348 ===\n",
            "train loss:1.9685300135253334\n",
            "train loss:1.9239930491765527\n",
            "train loss:1.9180888974182992\n",
            "=== epoch:156, train acc:0.5266666666666666, test acc:0.4401 ===\n",
            "train loss:1.920696872592509\n",
            "train loss:1.8801162637440965\n",
            "train loss:1.9389487715367721\n",
            "=== epoch:157, train acc:0.54, test acc:0.4425 ===\n",
            "train loss:1.95828220977766\n",
            "train loss:1.8958640906777393\n",
            "train loss:1.9266973032464076\n",
            "=== epoch:158, train acc:0.5433333333333333, test acc:0.4425 ===\n",
            "train loss:1.8970423765309068\n",
            "train loss:1.9366945792853667\n",
            "train loss:1.8735277199392408\n",
            "=== epoch:159, train acc:0.54, test acc:0.4454 ===\n",
            "train loss:1.9527570728685122\n",
            "train loss:1.8887878352024814\n",
            "train loss:1.8919947611748165\n",
            "=== epoch:160, train acc:0.5566666666666666, test acc:0.4483 ===\n",
            "train loss:1.8638872324716689\n",
            "train loss:1.845523221475766\n",
            "train loss:1.8660231129292097\n",
            "=== epoch:161, train acc:0.5566666666666666, test acc:0.4515 ===\n",
            "train loss:1.8618657473705473\n",
            "train loss:1.8270536417875598\n",
            "train loss:1.8420325034477938\n",
            "=== epoch:162, train acc:0.56, test acc:0.452 ===\n",
            "train loss:1.8643140468783352\n",
            "train loss:1.8848067616817703\n",
            "train loss:1.9446397896532306\n",
            "=== epoch:163, train acc:0.57, test acc:0.4579 ===\n",
            "train loss:1.8600449058931645\n",
            "train loss:1.8524036598966815\n",
            "train loss:1.8500827232237986\n",
            "=== epoch:164, train acc:0.57, test acc:0.4564 ===\n",
            "train loss:1.9041026695021228\n",
            "train loss:1.9033913131490912\n",
            "train loss:1.8497649565120649\n",
            "=== epoch:165, train acc:0.56, test acc:0.462 ===\n",
            "train loss:1.9264483420463459\n",
            "train loss:1.9140388630474243\n",
            "train loss:1.8726215154545947\n",
            "=== epoch:166, train acc:0.5733333333333334, test acc:0.4714 ===\n",
            "train loss:1.8527595171490874\n",
            "train loss:1.8713245088888275\n",
            "train loss:1.7924827683914362\n",
            "=== epoch:167, train acc:0.5733333333333334, test acc:0.4695 ===\n",
            "train loss:1.9168007536408282\n",
            "train loss:1.8761368406495296\n",
            "train loss:1.8865626480960913\n",
            "=== epoch:168, train acc:0.57, test acc:0.4731 ===\n",
            "train loss:1.828788579709663\n",
            "train loss:1.8345161985357266\n",
            "train loss:1.841556852260272\n",
            "=== epoch:169, train acc:0.57, test acc:0.4706 ===\n",
            "train loss:1.8590344855268097\n",
            "train loss:1.9360007192632047\n",
            "train loss:1.7579129154271076\n",
            "=== epoch:170, train acc:0.5666666666666667, test acc:0.4716 ===\n",
            "train loss:1.8488694774968604\n",
            "train loss:1.8289844601958052\n",
            "train loss:1.8477949746140336\n",
            "=== epoch:171, train acc:0.5733333333333334, test acc:0.4723 ===\n",
            "train loss:1.8658109791321345\n",
            "train loss:1.849592443436796\n",
            "train loss:1.7420614599366386\n",
            "=== epoch:172, train acc:0.58, test acc:0.4772 ===\n",
            "train loss:1.9187675003518307\n",
            "train loss:1.853215351793543\n",
            "train loss:1.8286941947551887\n",
            "=== epoch:173, train acc:0.5966666666666667, test acc:0.4816 ===\n",
            "train loss:1.8202576775686665\n",
            "train loss:1.7586500311218196\n",
            "train loss:1.7741830256250468\n",
            "=== epoch:174, train acc:0.5966666666666667, test acc:0.4806 ===\n",
            "train loss:1.865594221210663\n",
            "train loss:1.8087104987548974\n",
            "train loss:1.8029929987641378\n",
            "=== epoch:175, train acc:0.5933333333333334, test acc:0.4834 ===\n",
            "train loss:1.8271550430873977\n",
            "train loss:1.824015391956615\n",
            "train loss:1.8462316250163047\n",
            "=== epoch:176, train acc:0.5966666666666667, test acc:0.4878 ===\n",
            "train loss:1.8068612407459028\n",
            "train loss:1.791352551949569\n",
            "train loss:1.7526325718322107\n",
            "=== epoch:177, train acc:0.6, test acc:0.4886 ===\n",
            "train loss:1.7058501899702418\n",
            "train loss:1.747161738131513\n",
            "train loss:1.7436801201494256\n",
            "=== epoch:178, train acc:0.6033333333333334, test acc:0.4865 ===\n",
            "train loss:1.7524531948704711\n",
            "train loss:1.7230084677118558\n",
            "train loss:1.7721454310845552\n",
            "=== epoch:179, train acc:0.6, test acc:0.4847 ===\n",
            "train loss:1.901454925629284\n",
            "train loss:1.7590024370567334\n",
            "train loss:1.76532109544789\n",
            "=== epoch:180, train acc:0.5933333333333334, test acc:0.4826 ===\n",
            "train loss:1.7084903971264378\n",
            "train loss:1.7973580274878365\n",
            "train loss:1.796264957566455\n",
            "=== epoch:181, train acc:0.6, test acc:0.4855 ===\n",
            "train loss:1.8152962227963299\n",
            "train loss:1.7791298084886342\n",
            "train loss:1.6968552345809058\n",
            "=== epoch:182, train acc:0.5933333333333334, test acc:0.4883 ===\n",
            "train loss:1.8017983250517258\n",
            "train loss:1.7866184488821142\n",
            "train loss:1.7688606963777196\n",
            "=== epoch:183, train acc:0.61, test acc:0.4966 ===\n",
            "train loss:1.7490696743845475\n",
            "train loss:1.5539050774398928\n",
            "train loss:1.8118753816343804\n",
            "=== epoch:184, train acc:0.6133333333333333, test acc:0.4973 ===\n",
            "train loss:1.7232025812929677\n",
            "train loss:1.6781921121817447\n",
            "train loss:1.7777341684299721\n",
            "=== epoch:185, train acc:0.6033333333333334, test acc:0.4987 ===\n",
            "train loss:1.771312535872928\n",
            "train loss:1.7548212662134788\n",
            "train loss:1.705710761794086\n",
            "=== epoch:186, train acc:0.61, test acc:0.5036 ===\n",
            "train loss:1.7076018802887127\n",
            "train loss:1.605587250812029\n",
            "train loss:1.7601679959990657\n",
            "=== epoch:187, train acc:0.6133333333333333, test acc:0.504 ===\n",
            "train loss:1.7638229924096689\n",
            "train loss:1.67259498026404\n",
            "train loss:1.694651167806032\n",
            "=== epoch:188, train acc:0.6133333333333333, test acc:0.501 ===\n",
            "train loss:1.6430232764347534\n",
            "train loss:1.7959297877803926\n",
            "train loss:1.7698044996316553\n",
            "=== epoch:189, train acc:0.6166666666666667, test acc:0.5013 ===\n",
            "train loss:1.727049571641902\n",
            "train loss:1.7073146522046883\n",
            "train loss:1.6763337552928272\n",
            "=== epoch:190, train acc:0.6166666666666667, test acc:0.5049 ===\n",
            "train loss:1.7776019170605624\n",
            "train loss:1.6920915893767934\n",
            "train loss:1.7186450020880293\n",
            "=== epoch:191, train acc:0.6166666666666667, test acc:0.5062 ===\n",
            "train loss:1.7076449521078185\n",
            "train loss:1.772459303581971\n",
            "train loss:1.716785843490182\n",
            "=== epoch:192, train acc:0.6133333333333333, test acc:0.504 ===\n",
            "train loss:1.6547780850338776\n",
            "train loss:1.6921960701806729\n",
            "train loss:1.6093048484950412\n",
            "=== epoch:193, train acc:0.6166666666666667, test acc:0.5072 ===\n",
            "train loss:1.5901565946225165\n",
            "train loss:1.7465555275138251\n",
            "train loss:1.6880068769753669\n",
            "=== epoch:194, train acc:0.62, test acc:0.5081 ===\n",
            "train loss:1.6662484531303647\n",
            "train loss:1.6564048277620698\n",
            "train loss:1.7145559743004222\n",
            "=== epoch:195, train acc:0.62, test acc:0.5116 ===\n",
            "train loss:1.5924223378854574\n",
            "train loss:1.6719011235660486\n",
            "train loss:1.6197715695354598\n",
            "=== epoch:196, train acc:0.6233333333333333, test acc:0.513 ===\n",
            "train loss:1.5899863087134296\n",
            "train loss:1.6834348995982842\n",
            "train loss:1.627069279049074\n",
            "=== epoch:197, train acc:0.6233333333333333, test acc:0.5119 ===\n",
            "train loss:1.5955487362091525\n",
            "train loss:1.6376524023767414\n",
            "train loss:1.6481152491164293\n",
            "=== epoch:198, train acc:0.62, test acc:0.5117 ===\n",
            "train loss:1.6332201716863033\n",
            "train loss:1.6175600641551628\n",
            "train loss:1.6232962182422275\n",
            "=== epoch:199, train acc:0.6233333333333333, test acc:0.5158 ===\n",
            "train loss:1.6572971853305045\n",
            "train loss:1.625314172016238\n",
            "train loss:1.673611340373678\n",
            "=== epoch:200, train acc:0.6266666666666667, test acc:0.5182 ===\n",
            "train loss:1.5141284758010556\n",
            "train loss:1.5820605375152912\n",
            "train loss:1.5478446868339857\n",
            "=== epoch:201, train acc:0.6233333333333333, test acc:0.517 ===\n",
            "train loss:1.576122253015805\n",
            "train loss:1.7375691621259783\n",
            "train loss:1.720030179482468\n",
            "=== epoch:202, train acc:0.62, test acc:0.5181 ===\n",
            "train loss:1.6500784926459107\n",
            "train loss:1.5887167180332202\n",
            "train loss:1.5867825945357004\n",
            "=== epoch:203, train acc:0.6233333333333333, test acc:0.5225 ===\n",
            "train loss:1.6351287606201952\n",
            "train loss:1.519614981221893\n",
            "train loss:1.5843462371539285\n",
            "=== epoch:204, train acc:0.6266666666666667, test acc:0.525 ===\n",
            "train loss:1.5879274043849987\n",
            "train loss:1.612374045997907\n",
            "train loss:1.6028033638708399\n",
            "=== epoch:205, train acc:0.6333333333333333, test acc:0.5291 ===\n",
            "train loss:1.57222564762743\n",
            "train loss:1.4947444541231372\n",
            "train loss:1.6355419073821351\n",
            "=== epoch:206, train acc:0.63, test acc:0.5271 ===\n",
            "train loss:1.6161245123900962\n",
            "train loss:1.5958139715384292\n",
            "train loss:1.6158842625757959\n",
            "=== epoch:207, train acc:0.6433333333333333, test acc:0.5313 ===\n",
            "train loss:1.543463125995903\n",
            "train loss:1.625807437432096\n",
            "train loss:1.5981923074248343\n",
            "=== epoch:208, train acc:0.65, test acc:0.5346 ===\n",
            "train loss:1.4561091702931597\n",
            "train loss:1.5491101535794078\n",
            "train loss:1.5520138235880319\n",
            "=== epoch:209, train acc:0.65, test acc:0.5338 ===\n",
            "train loss:1.5344525954175856\n",
            "train loss:1.5382417172283054\n",
            "train loss:1.536744664575043\n",
            "=== epoch:210, train acc:0.6633333333333333, test acc:0.5415 ===\n",
            "train loss:1.490482027378386\n",
            "train loss:1.6878257810522939\n",
            "train loss:1.5238135216483801\n",
            "=== epoch:211, train acc:0.66, test acc:0.543 ===\n",
            "train loss:1.537034137304355\n",
            "train loss:1.5916220691397829\n",
            "train loss:1.6397793345782776\n",
            "=== epoch:212, train acc:0.67, test acc:0.5463 ===\n",
            "train loss:1.620126734941945\n",
            "train loss:1.5813503411131062\n",
            "train loss:1.531302126772529\n",
            "=== epoch:213, train acc:0.6633333333333333, test acc:0.5443 ===\n",
            "train loss:1.327238608476419\n",
            "train loss:1.338725885053708\n",
            "train loss:1.635544189518216\n",
            "=== epoch:214, train acc:0.6566666666666666, test acc:0.5455 ===\n",
            "train loss:1.5915410137198955\n",
            "train loss:1.4673985846662077\n",
            "train loss:1.5025047613882598\n",
            "=== epoch:215, train acc:0.6533333333333333, test acc:0.5433 ===\n",
            "train loss:1.5506321898922062\n",
            "train loss:1.4874148698166942\n",
            "train loss:1.5694151663430154\n",
            "=== epoch:216, train acc:0.6733333333333333, test acc:0.55 ===\n",
            "train loss:1.4854175862733654\n",
            "train loss:1.583107506546452\n",
            "train loss:1.4687267654744296\n",
            "=== epoch:217, train acc:0.67, test acc:0.5513 ===\n",
            "train loss:1.5598459079184428\n",
            "train loss:1.5496536313585614\n",
            "train loss:1.4100161031065304\n",
            "=== epoch:218, train acc:0.6766666666666666, test acc:0.556 ===\n",
            "train loss:1.6755247368483277\n",
            "train loss:1.511849422809469\n",
            "train loss:1.4562824329832054\n",
            "=== epoch:219, train acc:0.6866666666666666, test acc:0.5591 ===\n",
            "train loss:1.479881939840107\n",
            "train loss:1.5164381054669283\n",
            "train loss:1.5434024494559275\n",
            "=== epoch:220, train acc:0.6833333333333333, test acc:0.5608 ===\n",
            "train loss:1.6476934076042544\n",
            "train loss:1.4328819266984842\n",
            "train loss:1.4904709739140223\n",
            "=== epoch:221, train acc:0.69, test acc:0.5603 ===\n",
            "train loss:1.5852022230954104\n",
            "train loss:1.4997240687535074\n",
            "train loss:1.4477796181653855\n",
            "=== epoch:222, train acc:0.6933333333333334, test acc:0.5643 ===\n",
            "train loss:1.5196177378624853\n",
            "train loss:1.482742809139323\n",
            "train loss:1.4136670906431978\n",
            "=== epoch:223, train acc:0.69, test acc:0.5622 ===\n",
            "train loss:1.4211088144911128\n",
            "train loss:1.453862567400852\n",
            "train loss:1.449518384608214\n",
            "=== epoch:224, train acc:0.6833333333333333, test acc:0.561 ===\n",
            "train loss:1.2592049077708354\n",
            "train loss:1.3580543070305453\n",
            "train loss:1.4276564091648243\n",
            "=== epoch:225, train acc:0.6833333333333333, test acc:0.5614 ===\n",
            "train loss:1.4972289827871745\n",
            "train loss:1.4093469026541672\n",
            "train loss:1.4312525430287375\n",
            "=== epoch:226, train acc:0.68, test acc:0.5582 ===\n",
            "train loss:1.3605422299911896\n",
            "train loss:1.329590572619065\n",
            "train loss:1.432521589094079\n",
            "=== epoch:227, train acc:0.68, test acc:0.5579 ===\n",
            "train loss:1.432747595578055\n",
            "train loss:1.474562056173811\n",
            "train loss:1.3336732678818048\n",
            "=== epoch:228, train acc:0.68, test acc:0.5568 ===\n",
            "train loss:1.5134376380522274\n",
            "train loss:1.4606853534723814\n",
            "train loss:1.293588248684331\n",
            "=== epoch:229, train acc:0.6833333333333333, test acc:0.5577 ===\n",
            "train loss:1.2865651593781626\n",
            "train loss:1.4078090436341706\n",
            "train loss:1.2185194579048757\n",
            "=== epoch:230, train acc:0.6766666666666666, test acc:0.5534 ===\n",
            "train loss:1.3668491976169093\n",
            "train loss:1.3281182357970343\n",
            "train loss:1.3816401488760492\n",
            "=== epoch:231, train acc:0.6833333333333333, test acc:0.5569 ===\n",
            "train loss:1.2729292472364\n",
            "train loss:1.3786723393094413\n",
            "train loss:1.4710467350112773\n",
            "=== epoch:232, train acc:0.6633333333333333, test acc:0.547 ===\n",
            "train loss:1.4334243310309063\n",
            "train loss:1.470154656468863\n",
            "train loss:1.392407918097473\n",
            "=== epoch:233, train acc:0.6566666666666666, test acc:0.5419 ===\n",
            "train loss:1.3166824092417735\n",
            "train loss:1.3374856328502123\n",
            "train loss:1.3371934002024373\n",
            "=== epoch:234, train acc:0.6566666666666666, test acc:0.5405 ===\n",
            "train loss:1.3522334205034314\n",
            "train loss:1.3784166518212861\n",
            "train loss:1.282916846839971\n",
            "=== epoch:235, train acc:0.6533333333333333, test acc:0.5403 ===\n",
            "train loss:1.4117221430395706\n",
            "train loss:1.2609564469267542\n",
            "train loss:1.3367450568477588\n",
            "=== epoch:236, train acc:0.6566666666666666, test acc:0.5404 ===\n",
            "train loss:1.394257020488649\n",
            "train loss:1.3656651805881665\n",
            "train loss:1.4323629988554927\n",
            "=== epoch:237, train acc:0.6533333333333333, test acc:0.5434 ===\n",
            "train loss:1.242874298048958\n",
            "train loss:1.4434821735973729\n",
            "train loss:1.2868579061174241\n",
            "=== epoch:238, train acc:0.66, test acc:0.5456 ===\n",
            "train loss:1.3087448802239046\n",
            "train loss:1.4174880343447376\n",
            "train loss:1.350211117909072\n",
            "=== epoch:239, train acc:0.6466666666666666, test acc:0.5409 ===\n",
            "train loss:1.3845515681308882\n",
            "train loss:1.3979772442880531\n",
            "train loss:1.36415733214742\n",
            "=== epoch:240, train acc:0.6666666666666666, test acc:0.5512 ===\n",
            "train loss:1.2656735059315434\n",
            "train loss:1.4491540539070957\n",
            "train loss:1.4169225945226562\n",
            "=== epoch:241, train acc:0.6733333333333333, test acc:0.5557 ===\n",
            "train loss:1.1903645725603178\n",
            "train loss:1.4229178123899677\n",
            "train loss:1.2905654491038918\n",
            "=== epoch:242, train acc:0.6833333333333333, test acc:0.5562 ===\n",
            "train loss:1.3157107706182172\n",
            "train loss:1.501659427605575\n",
            "train loss:1.2000884327434433\n",
            "=== epoch:243, train acc:0.6766666666666666, test acc:0.5561 ===\n",
            "train loss:1.1989124877176973\n",
            "train loss:1.3519071524958588\n",
            "train loss:1.3260891569322146\n",
            "=== epoch:244, train acc:0.6733333333333333, test acc:0.5531 ===\n",
            "train loss:1.3823589468766493\n",
            "train loss:1.263163625438918\n",
            "train loss:1.3045454285015137\n",
            "=== epoch:245, train acc:0.6866666666666666, test acc:0.56 ===\n",
            "train loss:1.252223873975436\n",
            "train loss:1.2519509842421448\n",
            "train loss:1.266892155027535\n",
            "=== epoch:246, train acc:0.6866666666666666, test acc:0.5605 ===\n",
            "train loss:1.3045333798432173\n",
            "train loss:1.2919436991218185\n",
            "train loss:1.387163775752586\n",
            "=== epoch:247, train acc:0.7066666666666667, test acc:0.5648 ===\n",
            "train loss:1.368658687296441\n",
            "train loss:1.2872590432628823\n",
            "train loss:1.35305171306247\n",
            "=== epoch:248, train acc:0.7066666666666667, test acc:0.5663 ===\n",
            "train loss:1.2742164401316065\n",
            "train loss:1.3301521270155041\n",
            "train loss:1.2940335459712435\n",
            "=== epoch:249, train acc:0.7166666666666667, test acc:0.568 ===\n",
            "train loss:1.2168613766954997\n",
            "train loss:1.3877959847033892\n",
            "train loss:1.2900293055138792\n",
            "=== epoch:250, train acc:0.71, test acc:0.5667 ===\n",
            "train loss:1.199036571663688\n",
            "train loss:1.2323303934806165\n",
            "train loss:1.2639177127776138\n",
            "=== epoch:251, train acc:0.71, test acc:0.5674 ===\n",
            "train loss:1.0807407855718574\n",
            "train loss:1.1669810097027167\n",
            "train loss:1.2832512174293838\n",
            "=== epoch:252, train acc:0.7133333333333334, test acc:0.5716 ===\n",
            "train loss:1.133763035301972\n",
            "train loss:1.1840002228626074\n",
            "train loss:1.3168606463457522\n",
            "=== epoch:253, train acc:0.7233333333333334, test acc:0.5709 ===\n",
            "train loss:1.2555755070559533\n",
            "train loss:1.1807728198777558\n",
            "train loss:1.3577899693553714\n",
            "=== epoch:254, train acc:0.7233333333333334, test acc:0.5691 ===\n",
            "train loss:1.1968455009796082\n",
            "train loss:1.0967551495459895\n",
            "train loss:1.2082210669646143\n",
            "=== epoch:255, train acc:0.7166666666666667, test acc:0.5684 ===\n",
            "train loss:1.30162836070725\n",
            "train loss:1.2279272411647393\n",
            "train loss:1.2122575170181127\n",
            "=== epoch:256, train acc:0.7166666666666667, test acc:0.5689 ===\n",
            "train loss:1.348333900994584\n",
            "train loss:1.3052996360413243\n",
            "train loss:1.1904575028098998\n",
            "=== epoch:257, train acc:0.6933333333333334, test acc:0.5659 ===\n",
            "train loss:1.1347533210278542\n",
            "train loss:1.2735673575480424\n",
            "train loss:1.163966836640362\n",
            "=== epoch:258, train acc:0.69, test acc:0.5656 ===\n",
            "train loss:1.309250594683186\n",
            "train loss:1.2510870897682198\n",
            "train loss:1.1486170805507916\n",
            "=== epoch:259, train acc:0.6933333333333334, test acc:0.569 ===\n",
            "train loss:1.2946569585241485\n",
            "train loss:1.1148530158178984\n",
            "train loss:1.3411741928550964\n",
            "=== epoch:260, train acc:0.69, test acc:0.5668 ===\n",
            "train loss:1.2494137229743358\n",
            "train loss:1.2092570417022188\n",
            "train loss:1.1776932227781325\n",
            "=== epoch:261, train acc:0.6966666666666667, test acc:0.568 ===\n",
            "train loss:1.0954331264147954\n",
            "train loss:1.1891251012162238\n",
            "train loss:1.2277719199245543\n",
            "=== epoch:262, train acc:0.69, test acc:0.5702 ===\n",
            "train loss:1.290986351296268\n",
            "train loss:1.280304852784353\n",
            "train loss:1.1646118328951205\n",
            "=== epoch:263, train acc:0.7166666666666667, test acc:0.572 ===\n",
            "train loss:1.2237149270653032\n",
            "train loss:1.3228544188069475\n",
            "train loss:1.2131031767354516\n",
            "=== epoch:264, train acc:0.71, test acc:0.5755 ===\n",
            "train loss:1.219669637541029\n",
            "train loss:1.1831879770558915\n",
            "train loss:1.214054192226795\n",
            "=== epoch:265, train acc:0.7233333333333334, test acc:0.5772 ===\n",
            "train loss:1.3192577353733566\n",
            "train loss:1.1989732833881421\n",
            "train loss:1.0673285095543457\n",
            "=== epoch:266, train acc:0.7333333333333333, test acc:0.5788 ===\n",
            "train loss:1.2543067506250283\n",
            "train loss:1.1686729640209577\n",
            "train loss:1.1557044196766184\n",
            "=== epoch:267, train acc:0.7333333333333333, test acc:0.5808 ===\n",
            "train loss:1.186520088043319\n",
            "train loss:1.2132407884534984\n",
            "train loss:1.0619404168110083\n",
            "=== epoch:268, train acc:0.7333333333333333, test acc:0.5802 ===\n",
            "train loss:1.2054925890683585\n",
            "train loss:1.1657470910797416\n",
            "train loss:1.0307382612450002\n",
            "=== epoch:269, train acc:0.7266666666666667, test acc:0.5818 ===\n",
            "train loss:1.1965887503639903\n",
            "train loss:1.2004876884298745\n",
            "train loss:1.2094476121808944\n",
            "=== epoch:270, train acc:0.74, test acc:0.582 ===\n",
            "train loss:1.1115873698920407\n",
            "train loss:1.1762586112380733\n",
            "train loss:1.118364558156392\n",
            "=== epoch:271, train acc:0.7333333333333333, test acc:0.5815 ===\n",
            "train loss:1.1139613549300251\n",
            "train loss:0.9522555795523246\n",
            "train loss:1.1441536120144034\n",
            "=== epoch:272, train acc:0.7333333333333333, test acc:0.5821 ===\n",
            "train loss:1.1456495427148774\n",
            "train loss:1.1739696557769255\n",
            "train loss:1.1206170059856773\n",
            "=== epoch:273, train acc:0.7266666666666667, test acc:0.5873 ===\n",
            "train loss:1.1439919203525364\n",
            "train loss:1.0918316748146568\n",
            "train loss:1.2181379469503852\n",
            "=== epoch:274, train acc:0.74, test acc:0.5846 ===\n",
            "train loss:1.0900280549009127\n",
            "train loss:1.0374166637894087\n",
            "train loss:1.1461602548598326\n",
            "=== epoch:275, train acc:0.7333333333333333, test acc:0.5845 ===\n",
            "train loss:1.0260137783695766\n",
            "train loss:1.0731554523729725\n",
            "train loss:1.1459452387326916\n",
            "=== epoch:276, train acc:0.7366666666666667, test acc:0.5852 ===\n",
            "train loss:1.1960770616056338\n",
            "train loss:1.2031635151521254\n",
            "train loss:1.084594681563091\n",
            "=== epoch:277, train acc:0.7366666666666667, test acc:0.5822 ===\n",
            "train loss:1.209094220451883\n",
            "train loss:1.2243832580585687\n",
            "train loss:1.1446908287045332\n",
            "=== epoch:278, train acc:0.7366666666666667, test acc:0.5802 ===\n",
            "train loss:1.190507408542056\n",
            "train loss:1.030955105418562\n",
            "train loss:1.082964751931064\n",
            "=== epoch:279, train acc:0.74, test acc:0.5826 ===\n",
            "train loss:1.0871818005982485\n",
            "train loss:1.152229851386923\n",
            "train loss:1.2640879393248388\n",
            "=== epoch:280, train acc:0.7366666666666667, test acc:0.5821 ===\n",
            "train loss:1.04610820756348\n",
            "train loss:0.9857748872281104\n",
            "train loss:1.0288526138865308\n",
            "=== epoch:281, train acc:0.7333333333333333, test acc:0.5799 ===\n",
            "train loss:1.066799422411562\n",
            "train loss:1.136283598449408\n",
            "train loss:1.0300725849823613\n",
            "=== epoch:282, train acc:0.7366666666666667, test acc:0.5809 ===\n",
            "train loss:1.0772405795557831\n",
            "train loss:1.0664019669637423\n",
            "train loss:1.0138001299206167\n",
            "=== epoch:283, train acc:0.73, test acc:0.5764 ===\n",
            "train loss:1.000089180486902\n",
            "train loss:1.1457081648606726\n",
            "train loss:1.14831910654189\n",
            "=== epoch:284, train acc:0.7333333333333333, test acc:0.579 ===\n",
            "train loss:1.1115839055276753\n",
            "train loss:0.887093699592541\n",
            "train loss:0.9256088300613983\n",
            "=== epoch:285, train acc:0.7266666666666667, test acc:0.5732 ===\n",
            "train loss:1.0880244484122892\n",
            "train loss:1.1794824901992227\n",
            "train loss:1.1386225439078526\n",
            "=== epoch:286, train acc:0.73, test acc:0.5764 ===\n",
            "train loss:1.0484384594214635\n",
            "train loss:0.9447785279129011\n",
            "train loss:1.051856913210028\n",
            "=== epoch:287, train acc:0.74, test acc:0.5804 ===\n",
            "train loss:1.0256334149350106\n",
            "train loss:1.03863436893065\n",
            "train loss:1.064567983527408\n",
            "=== epoch:288, train acc:0.7266666666666667, test acc:0.5754 ===\n",
            "train loss:1.0663904024952096\n",
            "train loss:1.1754538826857175\n",
            "train loss:1.1301839055011975\n",
            "=== epoch:289, train acc:0.7366666666666667, test acc:0.5804 ===\n",
            "train loss:1.148116737732447\n",
            "train loss:1.0643109336398917\n",
            "train loss:1.0907889224395666\n",
            "=== epoch:290, train acc:0.7366666666666667, test acc:0.5837 ===\n",
            "train loss:1.1714004084389587\n",
            "train loss:1.1135319682547762\n",
            "train loss:1.116693709119766\n",
            "=== epoch:291, train acc:0.7366666666666667, test acc:0.5828 ===\n",
            "train loss:0.9130772141519636\n",
            "train loss:0.9463579636017382\n",
            "train loss:0.9367469607162572\n",
            "=== epoch:292, train acc:0.7333333333333333, test acc:0.5839 ===\n",
            "train loss:0.9723730918866659\n",
            "train loss:1.0813422242267225\n",
            "train loss:1.0878735409053508\n",
            "=== epoch:293, train acc:0.7166666666666667, test acc:0.5808 ===\n",
            "train loss:1.2628579481308317\n",
            "train loss:1.0244327617466749\n",
            "train loss:1.0252224842709898\n",
            "=== epoch:294, train acc:0.75, test acc:0.5866 ===\n",
            "train loss:1.0201867407811918\n",
            "train loss:0.9386212169685887\n",
            "train loss:0.8464008883840727\n",
            "=== epoch:295, train acc:0.7566666666666667, test acc:0.5889 ===\n",
            "train loss:0.9177766975527558\n",
            "train loss:0.8990980166956297\n",
            "train loss:1.0686886814874395\n",
            "=== epoch:296, train acc:0.75, test acc:0.5896 ===\n",
            "train loss:0.9996773215963576\n",
            "train loss:1.017287571949613\n",
            "train loss:0.895893061597652\n",
            "=== epoch:297, train acc:0.7566666666666667, test acc:0.5918 ===\n",
            "train loss:0.987046332782437\n",
            "train loss:1.0588585111657491\n",
            "train loss:0.9715086399192472\n",
            "=== epoch:298, train acc:0.75, test acc:0.5908 ===\n",
            "train loss:1.0560963342782732\n",
            "train loss:0.9989854746437762\n",
            "train loss:1.0130593915942985\n",
            "=== epoch:299, train acc:0.76, test acc:0.5934 ===\n",
            "train loss:0.9775162705828231\n",
            "train loss:1.036642898689469\n",
            "train loss:1.0221827941844135\n",
            "=== epoch:300, train acc:0.75, test acc:0.5912 ===\n",
            "train loss:0.9025376425651206\n",
            "train loss:0.9772662005942783\n",
            "train loss:0.95718512193284\n",
            "=== epoch:301, train acc:0.7433333333333333, test acc:0.5886 ===\n",
            "train loss:0.9765680411176564\n",
            "train loss:0.999887140938569\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.5883\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}