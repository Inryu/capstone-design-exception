import tensorflow as tf

old_v = tf.logging.get_verbosity()
tf.logging.set_verbosity(tf.logging.ERROR)

from tensorflow.examples.tutorials.mnist import input_data
tf.set_random_seed(777)  # reproducibility

mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)


#x를 받아 y값(예측값)을 반환하는 함수
def layer(input, weight_shape, bias_shape):
    weight_stddev = (2.0/weight_shape[0])**0.5
    w_init = tf.random_normal_initializer(stddev=weight_stddev) 
    bias_init = tf.constant_initializer(value=0)
    W = tf.get_variable("W", weight_shape,initializer=w_init) 
    b = tf.get_variable("b", bias_shape,initializer=bias_init) 
    return tf.nn.relu(tf.matmul(input, W) + b)
    
    
#2개의 은닉층을 수행하는 함수    
def inference(x,keep_prob):
    
    x = tf.reshape(x, shape=[-1,28,28,1])    
    with tf.variable_scope("conv_1"):
        conv_1 = conv2d(x,[5,5,1,32],[32])
        pool_1 = max_pool2d(conv_1)
        
    with tf.variable_scope("conv_2"):
        conv_2 = conv2d(pool_1,[5,5,32,64],[64])
        pool_2 = max_pool2d(conv_2)
    
    with tf.variable_scope("fc"):
        pool_2_flat = tf.reshape(pool_2,[-1,7*7*64])
        fc_1  = layer(pool_2_flat,[7*7*64,1024],[1024])

        #dropout
        fc_1_drop = tf.nn.dropout(fc_1,keep_prob)
    
    with tf.variable_scope("output"):
        output = layer(fc_1_drop,[1024,10],[10])
        
    return output

def conv2d(input, weight_shape, bias_shape):
    insize = weight_shape[0]*weight_shape[1]*weight_shape[2]
    weight_init = tf.random_normal_initializer(stddev=(2.0/insize)**0.5)
    W = tf.get_variable("W",weight_shape,initializer=weight_init)

    bias_init = tf.constant_initializer(value=0)
    b = tf.get_variable("b",bias_shape,initializer=bias_init)

    conv_out = tf.nn.conv2d(input,W,strides=[1,1,1,1],padding='SAME')
    return tf.nn.relu(tf.nn.bias_add(conv_out,b))


def max_pool2d(input, k=2):
    return tf.nn.max_pool(input,ksize=[1,k,k,1],strides=[1,k,k,1],padding='SAME')

#출력층을 소프트맥스를 수행하며 오차를 계산하는 함수 
def loss(output, y):
    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y) 
    loss = tf.reduce_mean(xentropy)
    return loss
    

#정확성을 평가하는 함수
def evaluate(output, y):
    correct_prediction = tf.equal(tf.argmax(output, 1),tf.argmax(y, 1)) 
    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
    tf.summary.scalar("Validation Error:", (1-accuracy))
    return accuracy
    
    

# 경사하강법으로 학습을 진행하는 함수
def training(cost, global_step):
    tf.summary.scalar("cost", cost)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    train_op = optimizer.minimize(cost, global_step=global_step)
    return train_op
    
    
    
#실행부분
# 파라미터
learning_rate = 0.01 
training_epochs = 1000 
batch_size = 100 
display_step = 1

with tf.Graph().as_default():
    
    # mnist data image : 학습할 데이터 input shape 28*28=784
    #none은 minibatch의 사이즈 
    x = tf.placeholder("float", [None, 784])
    
    
    # 0-9 숫자인식 => 10개 클래스
    # 미니배치에 대한 정답 (만약 해당 그림이 0의 그림일 경우 y0:1 y1:0 y2:0 .... y9:0) 이게 1개에 대한 거고 미니배치니까 여러개
    y = tf.placeholder("float", [None, 10])

    keep_prob = tf.placeholder(tf.float32)
    
    
    
    output = inference(x,keep_prob)
    
    #실제 정답과 계산한 output오차계산 
    cost = loss(output, y) 
    
    
    global_step = tf.Variable(0, name='global_step',trainable=False) 
    train_op = training(cost, global_step)
    
    #정확도
    eval_op = evaluate(output, y)
    
    #시각화용도
    summary_op = tf.summary.merge_all()
    saver = tf.train.Saver()
    
    #세션시작 
    sess = tf.Session()
    summary_writer = tf.summary.FileWriter("logistic_logs2/",graph_def=sess.graph_def)
    
    
    #변수 초기화
    init_op = tf.global_variables_initializer() 
    sess.run(init_op)
    
    
    # 학습주기 
    for epoch in range(training_epochs):
        avg_cost = 0.
        
        # 데이터셋 ( 전체 데이터 양 ) / 배치사이즈( 한번에 넣어줄 데이터의 부분집합 사이즈 ) = 전체 데이터를 다 예제로 줘보려면 몇번넣어야하나
        total_batch = int(mnist.train.num_examples/batch_size) 
        
        # Loop over all batches
        # 전체 데이터를 전부 돌려봐야하니까 반복해야함 
        for i in range(total_batch):
            
            #미니배치로 x,y에 해당하는 데이터와 답으로 나눠서 학습 
            mbatch_x, mbatch_y = mnist.train.next_batch(batch_size)
            # 학습 하기 
            feed_dict = {x : mbatch_x, y : mbatch_y, keep_prob:0.5} 
            sess.run(train_op, feed_dict=feed_dict) #training 함수를 돌리려면 x,y가 필요 
            
            # 평균손실 계산
            minibatch_cost = sess.run(cost,feed_dict=feed_dict)
            avg_cost += minibatch_cost/total_batch
            
        # Display logs per epoch step 한 에폭시가 끝나면 검증데이터를 이용하여 오류율을 출력한다 
        if epoch % display_step == 0:
            val_feed_dict = {x : mnist.validation.images, y : mnist.validation.labels, keep_prob:1}
            accuracy = sess.run(eval_op,feed_dict=val_feed_dict)
            
            print ("Validation Error:",(1 - accuracy))
            
            #시각화를 위한 친구들 
            summary_str = sess.run(summary_op,feed_dict=feed_dict) 
            summary_writer.add_summary(summary_str,sess.run(global_step))
            saver.save(sess, "logistic_logs2/model-checkpoint", global_step=global_step)
            
            
        print ("Optimization Finished!")
        
        #검증이 끝나면 테스트 데이터를 통해 정확도를 출력한다 
        test_feed_dict = {x : mnist.test.images, y : mnist.test.labels, keep_prob:1}
        accuracy = sess.run(eval_op, feed_dict=test_feed_dict)
        print ("Test Accuracy:", accuracy)
