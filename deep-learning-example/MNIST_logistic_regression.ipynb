import tensorflow as tf old_v = tf.logging.get_verbosity() 

tf.logging.set_verbosity(tf.logging.ERROR) 
from tensorflow.examples.tutorials.mnist import input_data 
tf.set_random_seed(777) # reproducibility 

mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)



def inference(x): 
    init = tf.constant_initializer(value=0)
    W = tf.get_variable( "W", [784, 10],initializer=init) 
    b = tf.get_variable("b", [10],initializer=init) 
    output = tf.nn.softmax(tf.matmul(x, W) + b)
    return output


def loss(output, y):
    dot_product = y * tf.log(output)
    xentropy = -tf.reduce_sum(dot_product, reduction_indices=1)
    loss = tf.reduce_mean(xentropy) 
    return loss


def training(cost, global_step):
    #미니배치에 대한 비용, 검증오차, 파라미터의 분포를 기록
    tf.summary.scalar("cost", cost)
    
    #파라미터 최적화
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    train_op = optimizer.minimize(cost, global_step = global_step)
    
    return train_op


def evaluate(output, y): 
#검증데이터/테스트 데이터셋에 대한 평가를 위한 함수 
correct_prediction = tf.equal(tf.argmax(output, 1),tf.argmax(y, 1)) 
#정확도 계산 
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) 
return accuracy


# 파라미터 l
earning_rate = 0.01 
training_epochs = 1000 
batch_size = 100 
display_step = 1 




with tf.Graph().as_default(): 

# mnist data image : 학습할 데이터 input shape 28*28=784 #
none은 minibatch의 사이즈 
x = tf.placeholder("float", [None, 784]) 
# 0-9 숫자인식 => 10개 클래스 
y = tf.placeholder("float", [None, 10]) 
output = inference(x) 
#실제 정답과 계산한 output오차계산 
cost = loss(output, y) 
global_step = tf.Variable(0,name='global_step',trainable=False) train_op = training(cost, global_step) 
#정확도 
eval_op = evaluate(output, y) 
#시각화용도 
summary_op = tf.summary.merge_all() 
saver = tf.train.Saver() 

#세션시작 
sess = tf.Session() 
summary_writer = tf.summary.FileWriter("logistic_logs/",graph_def=sess.graph_def) 

#변수 초기화 
init_op = tf.global_variables_initializer() 
sess.run(init_op) 


# 학습주기 
for epoch in range(training_epochs): 
avg_cost = 0. 

# 데이터셋 ( 전체 데이터 양 ) / 배치사이즈( 한번에 넣어줄 데이터의 부분집합 사이즈 ) = 전체 데이터를 다 예제로 줘보려면 몇번넣어야하나 

total_batch = int(mnist.train.num_examples/batch_size) 
# Loop over all batches 
# 전체 데이터를 전부 돌려봐야하니까 반복해야함 
for i in range(total_batch): 
#미니배치로 x,y에 해당하는 데이터와 답으로 나눠서 학습 
mbatch_x, mbatch_y = mnist.train.next_batch(batch_size) 
# 학습 하기 
feed_dict = {x : mbatch_x, y : mbatch_y} 
sess.run(train_op, feed_dict=feed_dict) 
#training 함수를 돌리려면 x,y가 필요 
# 평균손실 계산 
minibatch_cost = sess.run(cost,feed_dict=feed_dict) 
avg_cost += minibatch_cost/total_batch 

# Display logs per epoch step 한 에폭시가 끝나면 검증데이터를 이용하여 오류율을 출력한다 
if epoch % display_step == 0: 
val_feed_dict = {x : mnist.validation.images, y : mnist.validation.labels} 
accuracy = sess.run(eval_op,feed_dict=val_feed_dict) 
print ("Validation Error:",(1 - accuracy)) 
#시각화를 위한 친구들 
summary_str = sess.run(summary_op,feed_dict=feed_dict) 
summary_writer.add_summary(summary_str,sess.run(global_step)) saver.save(sess, "logistic_logs/model-checkpoint", global_step=global_step) 


print ("Optimization Finished!") 
#검증이 끝나면 테스트 데이터를 통해 정확도를 출력한다 
test_feed_dict = {x : mnist.test.images, y : mnist.test.labels} 
accuracy = sess.run(eval_op, feed_dict=test_feed_dict) 
print ("Test Accuracy:", accuracy)
